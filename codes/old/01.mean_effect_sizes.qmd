---
title: "Meta-data Analysis"
format: html
editor: visual
---

This will describe different techniques that we used to summarize the meta data set

### Fist Approach: Fixed effect model (Unweighted)

In fixed effect analysis we assume that all included studies share a common effect size, ${\epsilon}$ . The observed effect size will be distributed about $\hat{\epsilon}$ with a variance $\sigma^2$ that depends primarily on the sample size for each study. Then the observed effect size $\hat\epsilon$ will be

$$
\hat\epsilon = {\epsilon}_i + e_i
$$

```{r, echo=FALSE}
# clear memory
rm(list = ls())
```

Note : No weighting has involved yet

```{r, echo= FALSE, warning=FALSE}
#| warning: false
## Load packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  dplyr,
  meta,
  metafor,
  tidyverse,
  robumeta,
  clubSandwich,
  pandoc,
  tinytex,
  modelsummary
)
df <- read.csv("./metadata/meta_dataset_water_clarity_TJ.csv")

# subset water clarity
df <- df[which(df$wqvar == "Water Clarity (Secchi depth)"),]

## Subset: Main models
# waterfront
df.wf <- df[which(df$distbuf == 1), ]
# non-waterfront
df.nwf <- df[which(df$distbuf == 2), ]

#define weights
df.wf.fe.u <- df.wf%>%
  mutate(var = 1, weight = 1) #do not consider variance into the accuracy of the 
df.nwf.fe.u <- df.nwf%>%
  mutate(var = 1, weight = 1) #do not consider variance into the accuracy of the 

#using metafor function
models <- list()
models[["Waterfront"]] <- rma(yi = wqelast, vi = var, data=df.wf.fe.u, method="EE",level = 95)
models[["Non-waterfront"]] <- rma(yi = wqelast, vi = var, data=df.nwf.fe.u, method="EE",level = 95)


modelsummary(models, fmt =4, stars = TRUE, coef_rename = c("overall" = "unweighted mean"), title = "Unweighted mean")

```

One common question about multivariate/multilevel (here multivariate means that single study contribute for multiple meta observation) is how such models assign weight to each individual effect size estimates. Let's take a brief look a basic fixed and random effects models, assuming that we have got a set of studies that each contribute a single effect size estimate so everything's independent. Letting $\hat{\epsilon}_i$ be the effect size from study $i$ with sampling variance ${V}_i$, for $i=1,...,k$ the basic fixed effect model is

$$
\hat{\epsilon}_i = {\epsilon}_i + e_i
$$ Then to estimate $\epsilon$ we take a weighted average of the effect size estimates:

$$
\hat{\epsilon}=\frac{1}W\sum_{j=1}^kw_i\epsilon_j
$$

where

$$
W={\sum_{j=1}^k}=w_j
$$ These weights are aiming to make overall estimates as precise as possible (i.e. having smallest possible sampling variance or standard error).Mathematically,the best weighting scheme may inverse variance weights that is setting the weights for each effect size estimate proportional to the inverse how much variance in their estimate. With inverse weights larger studies with precise effect size estimate will tend to get more weights and smaller, nosier studies will tend to get less weights.But when standard errors are not reported in the primary studies sample size weighting scheme also recommended

Thus we use set of weighting scheme to determine the effect size

### Second approach: Fixed effect model (Cluster weighted)

Cluster is defined as study & housing market combination which is similar to the Guignet's (2021) specification. Rationale behind this weighting scheme is that meta observations estimated from common transaction data set in-terms of the study area, time period, and water bodies are different estimates of the same underlying "true" elasticity. Within cluster average all the estimates and each cluster given a equal weight. More specifically;

$$
\bar{\epsilon_d}= \frac{{\sum_{j=1}^{k_d}}\sum_{i=1}^{k_{dj}}\frac{1}k_{dj}\hat{\epsilon_{idj}}}{K_d}
$$

where the same same weight ($w_{idj}=\frac{1}k_{dj}$) is given to each meta observation within cluster $j$ and for that distance bin. $K_d$ is the total number of clusters in the meta data set for distance bin $d$.

Note: Still variance or sample size does not involve with weighting

```{r, echo= FALSE, warning=FALSE}
# define weights
df.wf.fe.c <- df.wf%>%
  group_by(studyid, geog) %>%
  mutate(e_bar = mean(wqelast)) %>%
  group_by(studyid, geog) %>%
  mutate(cluster = cur_group_id()) %>%
  ungroup()%>%
  distinct(studyid,geog, .keep_all = TRUE)%>%
  mutate(var = 1)

df.nwf.fe.c <- df.nwf%>%
  group_by(studyid, geog, wqvar) %>%
  mutate(e_bar = mean(wqelast)) %>%
  group_by(studyid, geog) %>%
  mutate(cluster = cur_group_id()) %>%
  ungroup()%>%
  distinct(studyid,geog,wqvar, .keep_all = TRUE)%>%
  mutate(var = 1)

#using metafor function
models <- list()
models[["Waterfront"]] <- rma(yi = wqelast, vi = var, data=df.wf.fe.c, method="EE",level = 95)
models[["Nonwaterfront"]] <- rma(yi = wqelast, vi = var, data=df.nwf.fe.c, method="EE",level = 95)

rows <- tribble(~term, ~Waterfront, ~Nonwaterfront,
                "Number of observations", "398", "240",
               "Number of clusters", "119", "50")
attr(rows,'position') <- c(3,4)

modelsummary(models, fmt =4, stars = TRUE, coef_rename = c("overall" = "cluster adjusted mean"), title = "Cluster adjusted weighted mean", add_rows = rows, gof_omit = "Num.Obs")


```

### Second approach: Variacne adjusted cluste (VAC) weighted mean

Here adjustment has made to to derive a cluster weight based on inverse variance or sample size. Most of the primary studies often did not record the information to derive the standard error of the elasticity estimation, thus it has recommended to use sample size (with an assumption of standard errors are equally correlated). Here we have meta observations that lack in standard errors as well as the sample size (most of the observations are lack in standard error), thus I have calculated the mean elasticity using the weights based on both variance and sample size (for comparison and later we need to select one weight scheme based on information availability) It gives more weight to more precise estimation within a cluster but ensure that equal weight is given to each cluster (study+housing market). Variance between clusters are still did not considered.

The weight use for VAC is:

$$
w_{idj} = \frac{\frac{1}v_{idj}}{\sum_{idj}^{kd}\frac{1}v_{idj}}
$$

where $v_{idj}$ is variance of the esitmate $i$ in distance bin $d$, cluster $j$

The VAC weighted mean elasticity for distance bin $d$ is calculated as:

$$
\hat{\epsilon_d}=\frac{{\sum_{j=1}^{k_{d}}{\sum_{i=1}^{k_{dj}}}}\frac{\frac{1}v_{idj}}{\sum_{i=1}^{kdj}\frac{1}v_{idj}}{\hat{\epsilon_d}}}{K_d}
$$

```{r, echo=FALSE,warning=FALSE}
#waterfront
# define weights based on variance
df.wf.fe.vac <- df.wf%>%
  mutate(var = elast_sim_se^2)%>%
  group_by(studyid, geog) %>%
  mutate(v_bar = sum(var)) %>%
  ungroup()%>%
  mutate(weight = var/v_bar)%>%
  mutate(w_elas = wqelast*weight)%>%
  group_by(studyid, geog)%>%
  summarise_at(vars(w_elas), sum)%>%
  mutate(var = 1)

# define weights based on sample size

df.wf.fe.vac.ss <- df.wf%>%
  group_by(studyid, geog) %>%
  mutate(ss_bar = sum(sampsize)) %>%
  ungroup()%>%
  mutate(weight = sampsize/ss_bar)%>%
  mutate(w_elas = wqelast*weight)%>%
  group_by(studyid, geog)%>%
  summarise_at(vars(w_elas), sum)%>%
  mutate(var = 1)

#non-waterfront
# define weights based on variance

df.nwf.fe.vac <- df.nwf%>%
  mutate(var = elast_sim_se^2)%>%
  group_by(studyid, geog) %>%
  mutate(v_bar = sum(var)) %>%
  ungroup()%>%
  mutate(weight = var/v_bar)%>%
  mutate(w_elas = wqelast*weight)%>%
  group_by(studyid, geog)%>%
  summarise_at(vars(w_elas), sum)%>%
  mutate(var = 1)

# define weights based on sample size
df.nwf.fe.vac.ss <- df.nwf%>%
  group_by(studyid, geog) %>%
  mutate(ss_bar = sum(sampsize)) %>%
  ungroup()%>%
  mutate(weight = sampsize/ss_bar)%>%
  mutate(w_elas = wqelast*weight)%>%
  group_by(studyid, geog)%>%
  summarise_at(vars(w_elas), sum)%>%
  mutate(var = 1)

#using metafor function
models <- list()
models[["Waterfront"]] <- rma(yi = w_elas, vi = var, data=df.wf.fe.vac, method="EE",level = 95)
models[["Non-waterfront"]] <- rma(yi = w_elas, vi = var, data=df.nwf.fe.vac, method="EE",level = 95)

rows <- tribble(~term, ~Waterfront, ~Nonwaterfront,
                "Number of observations", "398", "240",
               "Number of clusters", "85", "19")
attr(rows,'position') <- c(3,4)

modelsummary(models, fmt =4, stars = TRUE, coef_rename = c("overall" = "variacne adjusted cluster weighted mean"), title = "VAC weighed mean basd on sample variace", gof_omit = "Num.Obs", add_rows = rows)
```

```{r, echo=FALSE,warning=FALSE}
models <- list()
models[["Waterfront"]] <- rma(yi = w_elas, vi = var, data=df.wf.fe.vac.ss, method="EE",level = 95)
models[["Non-Waterfront"]] <- rma(yi = w_elas, vi = var, data=df.nwf.fe.vac.ss, method="EE",level = 95)

rows <- tribble(~term, ~Waterfront, ~Nonwaterfront,
                "Number of observations", "398", "240",
               "Number of clusters", "91", "23")
attr(rows,'position') <- c(3,4)

modelsummary(models, fmt =4, stars = TRUE, coef_rename = c("overall" = "variacne adjusted cluster weighted mean"), title = "VAC weighed mean basd on sample size", gof_omit = "Num.Obs", add_rows = rows)

```

## Random effect models

In the basic random effect model, the weight for each study are proportional to

$$ 
w_{idj}=\frac{1}{\tau_{idj}^2+v_{idj}}
$$ Therefore in the random effect models, not only the sampling variance, but also the (estimated) amount of heterogeneity (i.e. variance in the underlying true effects) is taken into consideration when determining the weights. The denominator term here includes both the (estimated) between study heterogeneity and sampling variance because both term contribute to how noisy the effect size estimate is. (i.e. In fixed effect models above, we ignore between study heterogeneity and so weights are inversely proportional to the sampling variances with $w_{idj}=\frac{1}v_{idj}$). In random effect models, larger between-study heterogeneity will make the weights closer to equal, while smaller between-study heterogeneity will lead to weights that tend to emphasize larger studies with more precise estimations.This could be bit complicated in multivariate (i.e. each study contributes more than one observation into meta data set.)

The multivariate models fitted with "rma.mv()" function is typically be more complex than those fitted with rma() function. In particular, they will usually involve multiple random effects and possible correlated sampling errors. Here the model now considers three source of variability: Between-study heterogeneity ($\hat{\sigma}_{1}^2$), within study heterogeneity ($\hat{\sigma}_{2}^2$) and sampling variability ($v_{idj}$). The model-implied variences of the estimates are then sum of these three sources of variability (i.e.$\hat{\sigma}_{1}^2+\hat{\sigma}_{2}^2+v_{idj}$ ).

The model also implies a certain amount of covariance between the effects, which is also taken into consideration when computing the summary estimate.

First I estimate the effect size using "rma.mv" function and later explain how the weighting works

```{r, echo= FALSE, warning=FALSE}

df.wf.re <- df.wf%>%
  mutate(vi = elast_sim_se^2)%>%
  group_by(studyid, geog) %>%
  mutate(cluster = cur_group_id()) %>%
  ungroup()

df.nwf.re <- df.nwf%>%
  mutate(vi = elast_sim_se^2)%>%
  group_by(studyid, geog) %>%
  mutate(cluster = cur_group_id()) %>%
  ungroup()

#using metafor function
models <- list()
models[["Waterfront"]] <- rma.mv(yi = wqelast, vi, random = ~ 1 | cluster/obsid, data=df.wf.re)
models[["Non-waterfront"]] <- rma.mv(yi = wqelast, vi, random = ~ 1 | cluster/obsid, data=df.nwf.re)

rows <- tribble(~term, ~Waterfront, ~Nonwaterfront,
                "Number of observations", "235", "88",
               "Number of clusters", "88", "20")
attr(rows,'position') <- c(3,4)

modelsummary(models, fmt =4, stars = TRUE, coef_rename = c("overall" = "unweighted mean"), title = "RE size estimation with sample variacne", add_rows = rows, gof_omit = "Num.Obs")

```

If we pull the marginal variance-covariance (var-cov) matrix of the estimates (only first eight rows), it's looks like follows

For waterfront homes

```{r, echo=FALSE,warning=FALSE}
# obtain random effect weights to check how each cluster received weights

wf.re <- rma.mv(yi = wqelast, vi, random = ~ 1 | cluster/obsid, data=df.wf.re)
nwf.re <- rma.mv(yi = wqelast, vi, random = ~ 1 | cluster/obsid, data=df.nwf.re)

wi.wf <- 1 / (sum(wf.re$sigma2) + df.wf.re$vi)
wi.nwf <- 1 / (sum(nwf.re$sigma2) + df.nwf.re$vi)

# marginal variance-covariance (var-cov) matrix of the estimate (here the first 8 rows and columns) with

# var-cov - waterfront
wf.var.cov <- round(vcov(wf.re, type="obs")[1:8,1:8], 3)
wf.var.cov
```

For Non-waterfront homes

```{r, echo=FALSE,warning=FALSE}
# var-cov - non-waterfront
nwf.var.cov <- round(vcov(nwf.re, type="obs")[1:8,1:8], 3)
nwf.var.cov
```

The diagonal elements equal to $\hat{\sigma}_{1}^2+\hat{\sigma}_{2}^2+v_{idj}$ The off-diagnal elements in these blocks are covariances, which are equal to $\hat{\sigma}_{1}^2$ (i.e. the estimated between cluster variance component) in this type of model. As documented in help(rma.mv) this matrix is refereed as M (marginal var-cov matrix)

To compute the summary estimate, we need to take the inverse of this matrix. As a result, we not only have weights, but actually an entire weight matrix. We can inspect this matrix (or the first 8 rows and columns) with

waterfront homes

```{r, echo=FALSE,warning=FALSE}
#waterfront
round(weights(wf.re, type="matrix")[1:8,1:8], 3)
```

non-waterfront homes

```{r, echo=FALSE,warning=FALSE}
#non-waterfront
round(weights(nwf.re, type="matrix")[1:8,1:8], 3)
```

Since var-cov matrix is "block-diagonal" each block in the weight matrix is actually the inverse of the corresponding block from M matrix. Let $W=M^{-1}$ denote the weight matrix.

BY examining the weight matrix, we can also build some intuition how the model accounts for multiple estimates coming fro the same cluster. Note that off-diagonal elements in the blocks of the weight matrix are negative. This result in a certain amount of down weighting of these estimates, so that clusters contributing many estimates do not automatically receive tons of weight when computing summary estimates. To illustrate this we can compute sum of the row-sum weights within each cluster, in addition the table below shows how many estimates each cluster contributes.

```{r, echo=FALSE,warning=FALSE}
w <- weights(wf.re, type="matrix")


# To extract the weights
w.wf <- data.frame(k=c(table(df.wf.re$cluster)),
           weight = tapply(wi.wf,df.wf.re$cluster, sum))
w.nwf <- data.frame(k=c(table(df.nwf.re$cluster)),
           weight = tapply(wi.nwf,df.nwf.re$cluster, sum))

```

If we do the same random effect size estimation using sample size are accuracy measure instead of variance the following results can be obtained.

```{r, echo=FALSE, warning=FALSE}

df.wf.re.ss <- df.wf%>%
  group_by(studyid, geog) %>%
  mutate(cluster = cur_group_id()) %>%
  ungroup()%>%
  mutate(vi = sampsize)

df.nwf.re.ss <- df.nwf%>%
  group_by(studyid, geog) %>%
  mutate(cluster = cur_group_id()) %>%
  ungroup()%>%
  mutate(vi = sampsize)

#using metafor function
models <- list()
models[["Waterfront"]] <- rma.mv(yi = wqelast, vi, random = ~ 1 | cluster/obsid, data=df.wf.re.ss)
models[["Non-waterfront"]] <- rma.mv(yi = wqelast, vi, random = ~ 1 | cluster/obsid, data=df.nwf.re.ss)

rows <- tribble(~term, ~Waterfront, ~Nonwaterfront,
                "Number of observations", "290", "132",
               "Number of clusters", "91", "23")
attr(rows,'position') <- c(3,4)

modelsummary(models, fmt =4, stars = TRUE, coef_rename = c("overall" = "unweighted mean"), gof_omit = "Num.Obs", title = "RE size estimation with sample size", add_rows = rows)


```

Interestingly when we use sample size as accuracy measure of the estimation (instead of variance), it has changed the estimation a lot as well as the significance of the parameter also lost. The estimated effect size with sample variance is quite consistent with Guigniet's estimation while it has deviated when we use sample size.

Finally there are some meta observations that we need to get the information for calculation of variacne/ sample size and the results are based upon the complete data at the moment.
